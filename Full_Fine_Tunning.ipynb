{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP01C9YEqRl4ybPC5ZGdNu8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hadeel-77/LLM/blob/main/Full_Fine_Tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwSLbgyTf--b"
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets tensorflow scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x5ycOpDAgAsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Tweets.csv')"
      ],
      "metadata": {
        "id": "Vco-5sGdgTbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data Preprocessing"
      ],
      "metadata": {
        "id": "U_EQ2MpROUxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Keep only positive & negative samples\n",
        "df = df[df['airline_sentiment'].isin(['positive', 'negative'])].reset_index(drop=True)\n",
        "\n",
        "# Label encode: negative=0, positive=1\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['airline_sentiment'])\n",
        "\n",
        "# Split texts and labels\n",
        "\n",
        "train_texts, test_texts, y_train, y_test = train_test_split(\n",
        "    df['text'].tolist(),\n",
        "    df['label'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "dq5K1pmEOdnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Text Tokenization"
      ],
      "metadata": {
        "id": "0JQzXQV1OjNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize texts (pad & truncate to max_len=128)\n",
        "# Padding => Used when a text is shorter than the specified max_length,Adds special [PAD] tokens to the end of the input\n",
        "# so that all sequences have the same length.\n",
        "\n",
        "# Truncate => Used when a text is longer than max_length, It cuts off tokens beyond the specified maximum.\n",
        "\n",
        "\n",
        "train_encodings = tokenizer(\n",
        "    train_texts,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "test_encodings = tokenizer(\n",
        "    test_texts,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")"
      ],
      "metadata": {
        "id": "IEj_ExR9Or0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 : Invoke The Model , Feed The model The Inputs & Extract The CLS Tag"
      ],
      "metadata": {
        "id": "SVTQGikiO5tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load HuggingFace pre-trained BERT base model, we will use it as feature extractor,no classification head\n",
        "\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function that extract CLS tag\n",
        "\n",
        "def bert_encode(inputs):\n",
        "    input_ids, attention_mask = inputs\n",
        "    outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token , summarize the sentence embeddings\n",
        "    return cls_output\n",
        "\n",
        "# Define model inputs\n",
        "\n",
        "input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
        "cls_token = Lambda(bert_encode, output_shape=(768,))([input_ids, attention_mask])"
      ],
      "metadata": {
        "id": "xI4poyGMPNMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 4 : Adding our own classification layer to perform specilized task"
      ],
      "metadata": {
        "id": "upjoJjJGVzsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Add classification head\n",
        "x = Dropout(0.3)(cls_token) #drop 30% of the inputs to reduce overfitting\n",
        "x = Dense(64, activation='relu')(x) # 64 nurons to learn features\n",
        "x = Dropout(0.2)(x) # drop 20% to reduce overfitting in an intermediate layer\n",
        "output = Dense(1, activation='sigmoid')(x)  # Sinle nueron classify either 0 or 1"
      ],
      "metadata": {
        "id": "BQ8J0xT6WBpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ECayjDrvWHkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 : Build, Compile & Train The Model"
      ],
      "metadata": {
        "id": "Uox88VrSZ10h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build & compile the model\n",
        "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    x={'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
        "    y=y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=3,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "OzUydbio5wPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 : Evaluate The Model"
      ],
      "metadata": {
        "id": "EL5vM6XV54d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(\n",
        "    x={'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask']},\n",
        "    y=y_test\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Test Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "hrrxDjeNhDsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 : Build Prediction Function"
      ],
      "metadata": {
        "id": "KwcMvB1kF2bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, tokenizer, model):\n",
        "    # Tokenize input\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # Run prediction\n",
        "    prediction = model.predict({\n",
        "        'input_ids': encoding['input_ids'],\n",
        "        'attention_mask': encoding['attention_mask']\n",
        "    })\n",
        "\n",
        "    # Get probability\n",
        "    prob = float(prediction[0][0])\n",
        "\n",
        "    # Convert to label\n",
        "    label = \"positive\" if prob >= 0.5 else \"negative\"\n",
        "    return label, prob\n"
      ],
      "metadata": {
        "id": "yNGOUD26ipd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_1 = \"I love flying with this airline. Always a great experience!\"\n",
        "example_2 = \"Terrible service, I will never book with them again.\"\n",
        "\n",
        "print(predict_sentiment(example_1, tokenizer, model))  # ➜ ('positive', 0.87)\n",
        "print(predict_sentiment(example_2, tokenizer, model))  # ➜ ('negative', 0.12)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfGUtfGwudP3",
        "outputId": "c083ac78-f131-4fdd-824d-181402dd24b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n",
            "('positive', 0.5715762376785278)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "('negative', 0.15400893986225128)\n"
          ]
        }
      ]
    }
  ]
}